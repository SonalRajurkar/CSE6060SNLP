{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN NNP\n",
      "is AUX VBZ\n",
      "looking VERB VBG\n",
      "at ADP IN\n",
      "buying VERB VBG\n",
      "U.K. PROPN NNP\n",
      "startup NOUN NN\n",
      "for ADP IN\n",
      "$ SYM $\n",
      "1 NUM CD\n",
      "billion NUM CD\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog cat 0.4192831\n",
      "dog banana 0.4178361\n",
      "cat dog 0.4192831\n",
      "cat cat 1.0\n",
      "cat banana 0.34277543\n",
      "banana dog 0.4178361\n",
      "banana cat 0.34277543\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple dog 0.32773152\n",
      "Apple cat 0.25476366\n",
      "Apple banana 0.23497823\n",
      "is dog -0.008083976\n",
      "is cat -0.057422824\n",
      "is banana 0.2732478\n",
      "looking dog -0.012984838\n",
      "looking cat -0.003863984\n",
      "looking banana -0.13818161\n",
      "at dog -0.18504398\n",
      "at cat -0.14047386\n",
      "at banana -0.08360895\n",
      "buying dog 0.07290933\n",
      "buying cat 0.009232596\n",
      "buying banana -0.0062926114\n",
      "U.K. dog 0.17788927\n",
      "U.K. cat 0.31159577\n",
      "U.K. banana 0.29496405\n",
      "startup dog -0.07972747\n",
      "startup cat 0.16329421\n",
      "startup banana 0.21012439\n",
      "for dog 0.004944422\n",
      "for cat -0.112345\n",
      "for banana -0.17805888\n",
      "$ dog 0.03898256\n",
      "$ cat 0.16241148\n",
      "$ banana -0.030133458\n",
      "1 dog -0.025392925\n",
      "1 cat 0.042374816\n",
      "1 banana 0.059217025\n",
      "billion dog -0.09819397\n",
      "billion cat 0.05107036\n",
      "billion banana 0.1397048\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK SpaCy 0.5727981\n",
      "NLTK is -0.09461399\n",
      "NLTK fun 0.005918526\n",
      "is SpaCy 0.033348154\n",
      "is is 1.0\n",
      "is fun 0.05012374\n",
      "fun SpaCy 0.11429077\n",
      "fun is 0.11023276\n",
      "fun fun 1.0\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(\"NLTK is fun\")\n",
    "doc2=nlp(\"SpaCy is fun\")\n",
    "for token1 in doc1:\n",
    "    for token2 in doc2:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Orth Prefix Suffix Isalpha isDigit\n",
      "I 4690420944186131903 I I True False\n",
      "love 3702023516439754181 l ove True False\n",
      "singing 13402777656386554723 s ing True False\n",
      "20 2767521681098075859 2 20 False True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love singing 20\")\n",
    "print(\"Text Orth Prefix Suffix Isalpha isDigit\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]  #Each entry in vocab is lexeme\n",
    "    print(lexeme.text, lexeme.orth, lexeme.prefix_, lexeme.suffix_, #suffix by default takes last 3 characters\n",
    "            lexeme.is_alpha, lexeme.is_digit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
