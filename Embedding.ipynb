{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix.\n",
    "### word2vec is used for semantic (closely related items together) and syntactic (sequence) matching. Using word2vec, one can find similar words, dissimilar words, dimensional reduction, and many others. Another important feature of word2vec is to convert the higher dimensional representation of the text into lower dimensional of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\" Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship. Neural networks do not understand text instead they understand only numbers. Word Embedding provides a way to convert text to a numeric vector.\n",
    "\n",
    "Word2vec reconstructs the linguistic context of words. Before going further let us understand, what is linguistic context? In general life scenario when we speak or write to communicate, other people try to figure out what is objective of the sentence. For example, \"What is the temperature of India\", here the context is the user wants to know \"temperature of India\" which is context. In short, the main objective of a sentence is context. Word or sentence surrounding spoken or written language (disclosure) helps in determining the meaning of context. Word2vec learns vector representation of words through the contexts.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces escape character with space \n",
    "f = sample.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship. Neural networks do not understand text instead they understand only numbers. Word Embedding provides a way to convert text to a numeric vector.  Word2vec reconstructs the linguistic context of words. Before going further let us understand, what is linguistic context? In general life scenario when we speak or write to communicate, other people try to figure out what is objective of the sentence. For example, \"What is the temperature of India\", here the context is the user wants to know \"temperature of India\" which is context. In short, the main objective of a sentence is context. Word or sentence surrounding spoken or written language (disclosure) helps in determining the meaning of context. Word2vec learns vector representation of words through the contexts.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower())   #converting to lowercase\n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['word2vec', 'represents', 'words', 'in', 'vector', 'space', 'representation', '.'], ['words', 'are', 'represented', 'in', 'the', 'form', 'of', 'vectors', 'and', 'placement', 'is', 'done', 'in', 'such', 'a', 'way', 'that', 'similar', 'meaning', 'words', 'appear', 'together', 'and', 'dissimilar', 'words', 'are', 'located', 'far', 'away', '.'], ['this', 'is', 'also', 'termed', 'as', 'a', 'semantic', 'relationship', '.'], ['neural', 'networks', 'do', 'not', 'understand', 'text', 'instead', 'they', 'understand', 'only', 'numbers', '.'], ['word', 'embedding', 'provides', 'a', 'way', 'to', 'convert', 'text', 'to', 'a', 'numeric', 'vector', '.'], ['word2vec', 'reconstructs', 'the', 'linguistic', 'context', 'of', 'words', '.'], ['before', 'going', 'further', 'let', 'us', 'understand', ',', 'what', 'is', 'linguistic', 'context', '?'], ['in', 'general', 'life', 'scenario', 'when', 'we', 'speak', 'or', 'write', 'to', 'communicate', ',', 'other', 'people', 'try', 'to', 'figure', 'out', 'what', 'is', 'objective', 'of', 'the', 'sentence', '.'], ['for', 'example', ',', '``', 'what', 'is', 'the', 'temperature', 'of', 'india', \"''\", ',', 'here', 'the', 'context', 'is', 'the', 'user', 'wants', 'to', 'know', '``', 'temperature', 'of', 'india', \"''\", 'which', 'is', 'context', '.'], ['in', 'short', ',', 'the', 'main', 'objective', 'of', 'a', 'sentence', 'is', 'context', '.'], ['word', 'or', 'sentence', 'surrounding', 'spoken', 'or', 'written', 'language', '(', 'disclosure', ')', 'helps', 'in', 'determining', 'the', 'meaning', 'of', 'context', '.'], ['word2vec', 'learns', 'vector', 'representation', 'of', 'words', 'through', 'the', 'contexts', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "#Prints tokens for every sentence..There are 12 sentences in text so 12 arrrays are generated in the main array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of word2vec:\n",
    "### 1) CBOW     2)Skip-gram \n",
    "\n",
    "#### Skip-gram and CBOW convert unsupervised representation to supervised form for model training.\n",
    "#### In CBOW, the current word is predicted using the window of surrounding context windows.\n",
    "#### Skip-Gram performs opposite of CBOW which implies that it predicts the given sequence or context from the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model  (Continous Bag of Words)\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "#first parameter is the tokenized data \n",
    "#second parameter tells that it will ignore all the words with a total frequency lower than this.\n",
    "#Size tells the dimensionality of the word vectors \n",
    "#Maximum distance between the current and predicted word within a sentence is given by window.\n",
    "#window are mutable can be changed and accordingly can vary cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'word2vec' and 'vector' - CBOW :  0.22827439\n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between 'word2vec' \" + \n",
    "               \"and 'vector' - CBOW : \", \n",
    "    model1.similarity('word2vec', 'vector')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2737612e-03 -3.3196187e-04  1.8430880e-03 -1.1446645e-03\n",
      " -1.7483812e-03  2.1176853e-03  3.1230367e-05 -1.8511017e-04\n",
      " -1.5797671e-03  4.4432711e-03  6.4991799e-04  3.7352688e-04\n",
      " -2.6550293e-03 -1.9726602e-03 -3.8098360e-03 -5.5754028e-04\n",
      "  2.5970479e-03  4.4397218e-04  1.8673867e-03  1.4980830e-03\n",
      " -5.5223657e-04  2.1636818e-04  2.1586348e-03  3.5639561e-03\n",
      " -1.4623078e-03 -4.7500846e-03  3.8055563e-03  2.0230834e-03\n",
      "  1.5285627e-03 -4.1268342e-03 -2.2883867e-03  1.8498213e-03\n",
      " -6.5290520e-04 -2.1653136e-03 -1.6062878e-03  2.0962527e-03\n",
      " -3.2103178e-03  3.8158756e-03  4.3343212e-03 -8.8505720e-04\n",
      "  4.3593780e-03 -2.8637296e-03 -2.2938501e-04  5.5735005e-04\n",
      " -4.6465552e-04  3.2901354e-03  4.5688311e-03  4.0289501e-04\n",
      " -3.8200894e-03 -3.6102720e-03  4.2836759e-03 -2.3915959e-03\n",
      "  8.1738521e-04  1.9635456e-03  3.6609645e-03 -3.4428877e-03\n",
      " -4.9864231e-03  3.0929032e-03  4.5460057e-03 -2.8903047e-03\n",
      "  1.7890721e-03 -3.7593528e-04 -1.6116655e-04 -1.5436831e-03\n",
      " -1.1730894e-03 -3.4094923e-03  4.9948797e-04  4.0648752e-03\n",
      " -3.2626649e-03 -4.7062845e-03 -3.4043759e-03 -1.7086585e-03\n",
      "  1.0768133e-03 -1.1183090e-03 -3.6457586e-03  2.5477675e-03\n",
      " -3.9002988e-03 -1.5395870e-03 -1.0535602e-03 -2.3824240e-03\n",
      " -3.1626562e-03 -1.4826511e-03 -2.7429538e-03 -3.0938543e-03\n",
      "  1.2970688e-04  7.0713391e-04 -4.9953796e-03  2.5012346e-03\n",
      " -4.7513228e-04  1.4734652e-03 -2.8849831e-03  4.8544924e-03\n",
      " -2.4631945e-04 -3.6132354e-03  6.0734636e-04  4.5463967e-04\n",
      " -2.6368350e-03 -2.6089577e-03 -1.6607052e-04 -3.1059026e-03]\n"
     ]
    }
   ],
   "source": [
    "print(model1['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('written', 0.2410072684288025), ('this', 0.24062633514404297), ('represented', 0.2244727909564972), ('instead', 0.17960643768310547), ('such', 0.16462530195713043), (',', 0.16017262637615204), ('convert', 0.15883475542068481), ('try', 0.15453214943408966), ('vector', 0.15447998046875), ('we', 0.1442028284072876)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model1.most_similar('word')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissimilar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are\n"
     ]
    }
   ],
   "source": [
    "dissimlar_words = model1.doesnt_match('Words are represented in the form of vectors'.split())\n",
    "print(dissimlar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
